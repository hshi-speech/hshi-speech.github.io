---
title: "Speech Emotion Recognition with Multi-level Acoustic and Semantic Information Extraction and Interaction"
collection: publications
category: conferences
permalink: /publication/Interspeech2024gao
excerpt: 'Yuan Gao, Hao Shi, Chenhui Chu, and Tatsuya Kawahara'
date: 2024-09-01
venue: 'Interspeech'
biburl: 'https://raw.githubusercontent.com/hshi-speech/hshi_speech.github.io/master/files/bib/interspeech-2024-gao.txt'
paperurl: 'https://drive.google.com/file/d/1atWBcMsC4IdSj817UgnvZXBEin1omy05/view?usp=drive_link'
citation: #
---

Speech emotion recognition (SER) systems can learn linguistic information by integrating automatic speech recognition (ASR). However, existing SER systems fall short in explicitly learning semantic emotional information from ASR predictions. Our proposed system addresses this problem by incorporating a semantic feature extractor for explicit emotional information extraction. Furthermore, a cross attention-based information interaction module is proposed to learn the complementary emotional information in the embeddings from both feature extractors. Within the interaction module, a temporal-aware gate fusion network is incorporated to dynamically integrate the embeddings from acoustic and semantic feature extractors and mitigate the impact of ASR errors in SER. Experimental results on IEMOCAP show that our system outperforms the existing SER systems by improving the unweighted accuracy by 3.32%.
