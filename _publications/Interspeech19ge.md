---
title: "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement"
collection: publications
category: conferences
permalink: /publication/Interspeech19ge
excerpt: 'Meng Ge, Longbiao Wang, Nan Li, Hao Shi, Jianwu Dang, and Xiangang Li'
date: 2019-09-15
venue: 'Interspeech'
biburl: 'https://hshi_speech.github.io/tree/master/files/bib/interspeech-2019-ge.txt'
paperurl: 'https://github.com/hshi-speech/hshi_speech.github.io/blob/master/files/publications/interspeech-2019-ge.pdf'
citation: 'Meng Ge, Longbiao Wang, Nan Li, Hao Shi, Jianwu Dang, and Xiangang Li, "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement," in Proc. of Interspeech 2019, 3153-3157.'
---

Speech enhancement aims to keep the real speech signal and reduce noise for building robust communication systems. Under the success of DNN, significant progress has been made. Nevertheless, accuracy of the speech enhancement system is not satisfactory due to insufficient consideration of varied environmental and contextual information in complex cases. To address these problems, this research proposes an end-to-end environment-dependent attention-driven approach. The local frequency-temporal pattern via convolutional neural network is fully employed without pooling operation. It then integrates an attention mechanism into bidirectional long short-term memory to acquire the weighted dynamic context between consecutive frames. Furthermore, dynamic environment estimation and phase correction further improve the generalization ability. Extensive experimental results on REVERB challenge demonstrated that the proposed approach outperformed existing methods, improving PESQ from 2.56 to 2.87 and SRMR from 4.95 to 5.50 compared with conventional DNN.
